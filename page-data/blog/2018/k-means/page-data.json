{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/2018/k-means/","result":{"data":{"site":{"siteMetadata":{"title":"Roman Klimenko"}},"markdownRemark":{"id":"a6bee193-b55d-5883-a658-0c64af1b981b","excerpt":"Here is a quick and dirty implementation of the k-means clustering used to find a palette of dominant colours for an image. The code is hosted on the Observable…","html":"<p>Here is a quick and dirty implementation of the <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">k-means clustering</a> used to find a palette of dominant colours for an image.</p>\n<p>The code is hosted on the <a href=\"https://beta.observablehq.com/d/78d894babaef4084\">Observable</a> notebook.</p>\n<h2>Let’s take a picture</h2>\n<p>For example, let’s take this nice colourful photo taken by <a href=\"https://unsplash.com/@dylu\">Jacek Dylag</a> on <a href=\"https://unsplash.com/\">Unsplash</a>:</p>\n<img src=\"image.jpeg\" class=\"img-fluid\" />\n<p>To save the performance (remember, we run this code in the browser), let’s take a sample of 1000 random pixels of the image:</p>\n<img src=\"sample.png\" class=\"img-fluid\" />\n<p>It’s nearly impossible to guess the original image from these dots, but because they are randomly taken, we can use them as a sample data.</p>\n<p>The photo’s size is 600*399, which gives us 239400 pixels. Each pixel has 3 dimensions: red, green and blue (<a href=\"https://en.wikipedia.org/wiki/RGB_color_model\">RGB</a>) and can be represented as a vector:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">pixel = [R,G,B]</code></pre></div>\n<p>Let’s visualise the sample dots by drawing 2D projections of the 3D RGB color space:</p>\n<img src=\"rgb.png\" class=\"img-fluid\" />\n<h2>K is for cluster</h2>\n<blockquote>\n<p>k-means clustering is a method of vector quantisation, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">Wikipedia</a></p>\n</blockquote>\n<p>The classical k-means algorithm consists of the following steps:</p>\n<ol>\n<li>Take random k points (called centroids) from a space formed by our data points (i.e. vectors).</li>\n<li>Assign every data point to the closest centroid. Each centroid with assigned data point we call a cluster.</li>\n<li>For each cluster, find a new centroid by calculating a center between all the data points in the cluster.</li>\n<li>Repeat steps 2. and 3. while the coordinates of centroids change.</li>\n</ol>\n<p>Simple as a pie, isn’t it? Well, yes, but there are some nuances.</p>\n<h2>Performance</h2>\n<p>Say, <code class=\"language-text\">k</code> is a number of clusters (as well as centroids) and <code class=\"language-text\">n</code> is a number of data points, <code class=\"language-text\">d</code> is a number of dimensions (vector length) and <code class=\"language-text\">i</code> is a number of iterations (how many times 2. and 3. have to run). Roughly speaking, the complexity of this will be:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">O(k * n * d * i)</code></pre></div>\n<p>This can be pretty slow, and here are some simple ways to speed it up:</p>\n<ol>\n<li>Find the final centroids on the sample data and then run the last iteration on the full set. Usually, this significantly reduces the number of iterations on the full set.</li>\n<li>Set the top limit for the iterations number, so the method will not freeze forever.</li>\n<li>Set the minimal distance when centroids are considered to be the same. On my code, this saved 3–5 iterations when the distance is less than one but still slightly greater than zero.</li>\n<li>If you use Euclidean distance, don’t calculate the root, the squared distance is ok.</li>\n</ol>\n<h2>Distance and scale</h2>\n<ol>\n<li>Squared <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">Euclidean distance</a> is the simplest one, however, it may not be good to calculate the <a href=\"https://en.wikipedia.org/wiki/Color_difference\">color difference</a>.</li>\n<li>RGB color space is very simple, but again, if you really need to calculate the color difference precisely, use Lab and CIEDE2000.</li>\n</ol>\n<h2>The optimal number of k</h2>\n<p>To find an optimal <code class=\"language-text\">k</code>, I find the average variance for each cluster on the sample data. Simply put, a cluster’s variance is an average distance between its centroid and each point of the cluster. Therefore, the average variance of for a given <code class=\"language-text\">k</code> is the average variance of all its clusters. If we draw a chart where the variance is on the y-axis and <code class=\"language-text\">k</code> is the x-axis, we will see that the variance drops down, but at some point, the slope is decreasing significantly and after this value of k we can observe even some increasing of the variance:</p>\n<img src=\"variance.png\" class=\"img-fluid\" />\n<p>Now put this all together:</p>\n<ol>\n<li>Take a sample data set.</li>\n<li>Find centroids for different k on the sample data set.</li>\n<li>Find from which k the variance slows down its decrease.</li>\n<li>Run the k-means clustering with given initial centroids on the full data set.</li>\n</ol>\n<p>Voilà:</p>\n<img src=\"rgb2.png\" class=\"img-fluid\" />\n<p>The big circles represent centroids. The bigger a circle, the more data points are assigned to this centroid.</p>\n<p>And the posterised image:</p>\n<img src=\"result.png\" class=\"img-fluid\" />","frontmatter":{"title":"Dominant colors with k-means clustering","date":"May 25, 2018","tags":["k-means","clustering","dataisbeautiful"],"description":null}},"previous":{"fields":{"slug":"/blog/2018/maraphon/"},"frontmatter":{"title":"Marathon"}},"next":{"fields":{"slug":"/blog/2018/clt/"},"frontmatter":{"title":"Central limit theorem"}}},"pageContext":{"id":"a6bee193-b55d-5883-a658-0c64af1b981b","previousPostId":"fa4ac3f2-7a84-5c44-8a54-7dab45eac894","nextPostId":"640cbaa4-b863-51f1-8363-cb01deec8b0d"}},"staticQueryHashes":["2841359383"]}